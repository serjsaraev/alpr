{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2c37af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class PlatesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_path: str, json_path: str, sample_type: str='train', val_size: float=0.2, \n",
    "                random_state: int=42, transform: A.Compose=None):\n",
    "        \n",
    "        self.root_path = root_path\n",
    "        self.transfrom = transform\n",
    "        \n",
    "        with open(json_path, 'r') as f:\n",
    "            img_list = json.load(f)\n",
    "        \n",
    "        #img_list = img_list[:1000]\n",
    "        if sample_type == 'train':\n",
    "            self.img_list, _ = train_test_split(img_list, test_size=val_size, random_state=random_state)\n",
    "        elif sample_type == 'val':\n",
    "            _, self.img_list = train_test_split(img_list, test_size=val_size, random_state=random_state)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        try:\n",
    "            img = cv2.imread(os.path.join(self.root_path, self.img_list[idx]['file']))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        except:\n",
    "            idx = 0\n",
    "            img = cv2.imread(os.path.join(self.root_path, self.img_list[idx]['file']))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            print('Cant open image')\n",
    "        \n",
    "        objects = self.img_list[idx]['nums']\n",
    "        \n",
    "        bboxes = np.array([[min([i[0] for i in b['box']]), min([i[1] for i in b['box']]), \n",
    "                  max([i[0] for i in b['box']]), max([i[1] for i in b['box']])] for b in objects])\n",
    "        \n",
    "        for bbox in bboxes:\n",
    "            if (bbox[2] - bbox[0] < 0 ) or (bbox[3] - bbox[1] < 0):\n",
    "                print(idx)\n",
    "        \n",
    "        labels = np.ones(shape=(bboxes.shape[0]), dtype=np.int64)\n",
    "        \n",
    "        try:\n",
    "            if self.transfrom is not None:\n",
    "                sample = self.transfrom(image=img, bboxes=bboxes, labels=labels)\n",
    "                img, bboxes = sample['image'], sample['bboxes']\n",
    "            annot = np.hstack((bboxes, labels.reshape(labels.shape[0], 1)))\n",
    "        except:\n",
    "            print('Cant do augmention')\n",
    "            try:\n",
    "                sample = A.Compose([A.Resize(640, 640)],\n",
    "                                bbox_params={\n",
    "                                            'format': 'pascal_voc',\n",
    "                                            'label_fields': ['labels']\n",
    "                                            })(image=img, bboxes=bboxes, labels=labels)\n",
    "\n",
    "                img, bboxes = sample['image'], sample['bboxes']\n",
    "                annot = np.hstack((bboxes, labels.reshape(labels.shape[0], 1)))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                idx = 0\n",
    "                img = cv2.imread(os.path.join(self.root_path, self.img_list[idx]['file']))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                objects = self.img_list[idx]['nums']\n",
    "        \n",
    "                bboxes = np.array([[min([i[0] for i in b['box']]), min([i[1] for i in b['box']]), \n",
    "                          max([i[0] for i in b['box']]), max([i[1] for i in b['box']])] for b in objects])\n",
    "\n",
    "                for bbox in bboxes:\n",
    "                    if (bbox[2] - bbox[0] < 0 ) or (bbox[3] - bbox[1] < 0):\n",
    "                        print(idx)\n",
    "\n",
    "                labels = np.ones(shape=(bboxes.shape[0]), dtype=np.int64)\n",
    "                \n",
    "                if self.transfrom is not None:\n",
    "                    sample = self.transfrom(image=img, bboxes=bboxes, labels=labels)\n",
    "                    img, bboxes = sample['image'], sample['bboxes']\n",
    "            \n",
    "                annot = np.hstack((bboxes, labels.reshape(labels.shape[0], 1)))\n",
    "        \n",
    "        \n",
    "        sample = {'img': img, 'annot': annot}\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "        \n",
    "        \n",
    "def collate_fn(batch):\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "def collater(data):\n",
    "    imgs = [s['img'] for s in data]\n",
    "    annots = [torch.from_numpy(s['annot']) for s in data]\n",
    "\n",
    "    imgs = torch.from_numpy(np.stack(imgs, axis=0)).to(torch.float32)\n",
    "\n",
    "    max_num_annots = max(annot.shape[0] for annot in annots)\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "\n",
    "        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n",
    "\n",
    "        if max_num_annots > 0:\n",
    "            for idx, annot in enumerate(annots):\n",
    "                if annot.shape[0] > 0:\n",
    "                    annot_padded[idx, :annot.shape[0], :] = annot\n",
    "    else:\n",
    "        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n",
    "\n",
    "    imgs = imgs.permute(0, 3, 1, 2)\n",
    "\n",
    "    return {'img': imgs, 'annot': annot_padded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a7d334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def calc_iou(a, b):\n",
    "\n",
    "    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n",
    "    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n",
    "    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n",
    "    iw = torch.clamp(iw, min=0)\n",
    "    ih = torch.clamp(ih, min=0)\n",
    "    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n",
    "    ua = torch.clamp(ua, min=1e-8)\n",
    "    intersection = iw * ih\n",
    "    IoU = intersection / ua\n",
    "\n",
    "    return IoU\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FocalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, classifications, regressions, anchors, annotations):\n",
    "        alpha = 0.25\n",
    "        gamma = 2.0\n",
    "        batch_size = classifications.shape[0]\n",
    "        classification_losses = []\n",
    "        regression_losses = []\n",
    "\n",
    "        anchor = anchors[0, :, :]\n",
    "\n",
    "        anchor_widths = anchor[:, 2] - anchor[:, 0]\n",
    "        anchor_heights = anchor[:, 3] - anchor[:, 1]\n",
    "        anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths\n",
    "        anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights\n",
    "\n",
    "        for j in range(batch_size):\n",
    "\n",
    "            classification = classifications[j, :, :]\n",
    "            regression = regressions[j, :, :]\n",
    "            \n",
    "            bbox_annotation = annotations[j, :, :]\n",
    "            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n",
    "\n",
    "            if bbox_annotation.shape[0] == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    regression_losses.append(torch.tensor(0).float().cuda())\n",
    "                    classification_losses.append(torch.tensor(0).float().cuda())\n",
    "                else:\n",
    "                    regression_losses.append(torch.tensor(0).float())\n",
    "                    classification_losses.append(torch.tensor(0).float())\n",
    "\n",
    "                continue\n",
    "\n",
    "            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n",
    "\n",
    "            IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4])\n",
    "\n",
    "            IoU_max, IoU_argmax = torch.max(IoU, dim=1)\n",
    "\n",
    "            # compute the loss for classification\n",
    "            targets = torch.ones(classification.shape) * -1\n",
    "            if torch.cuda.is_available():\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            targets[torch.lt(IoU_max, 0.4), :] = 0\n",
    "\n",
    "            positive_indices = torch.ge(IoU_max, 0.5)\n",
    "\n",
    "            num_positive_anchors = positive_indices.sum()\n",
    "\n",
    "            assigned_annotations = bbox_annotation[IoU_argmax, :]\n",
    "\n",
    "            targets[positive_indices, :] = 0\n",
    "            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1\n",
    "\n",
    "            alpha_factor = torch.ones(targets.shape) * alpha\n",
    "            if torch.cuda.is_available():\n",
    "                alpha_factor = alpha_factor.cuda()\n",
    "\n",
    "            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n",
    "            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n",
    "            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n",
    "\n",
    "            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n",
    "\n",
    "            cls_loss = focal_weight * bce\n",
    "\n",
    "            zeros = torch.zeros(cls_loss.shape)\n",
    "            if torch.cuda.is_available():\n",
    "                zeros = zeros.cuda()\n",
    "            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)\n",
    "\n",
    "            classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.float(), min=1.0))\n",
    "\n",
    "\n",
    "            if positive_indices.sum() > 0:\n",
    "                assigned_annotations = assigned_annotations[positive_indices, :]\n",
    "\n",
    "                anchor_widths_pi = anchor_widths[positive_indices]\n",
    "                anchor_heights_pi = anchor_heights[positive_indices]\n",
    "                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n",
    "                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n",
    "\n",
    "                gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n",
    "                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n",
    "                gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths\n",
    "                gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights\n",
    "\n",
    "                gt_widths = torch.clamp(gt_widths, min=1)\n",
    "                gt_heights = torch.clamp(gt_heights, min=1)\n",
    "\n",
    "                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n",
    "                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n",
    "                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n",
    "                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n",
    "\n",
    "                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))\n",
    "                targets = targets.t()\n",
    "\n",
    "                norm = torch.Tensor([[0.1, 0.1, 0.2, 0.2]])\n",
    "                if torch.cuda.is_available():\n",
    "                    norm = norm.cuda()\n",
    "                targets = targets / norm\n",
    "\n",
    "                regression_diff = torch.abs(targets - regression[positive_indices, :])\n",
    "\n",
    "                regression_loss = torch.where(\n",
    "                    torch.le(regression_diff, 1.0 / 9.0),\n",
    "                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n",
    "                    regression_diff - 0.5 / 9.0\n",
    "                )\n",
    "                regression_losses.append(regression_loss.mean())\n",
    "            else:\n",
    "                if torch.cuda.is_available():\n",
    "                    regression_losses.append(torch.tensor(0).float().cuda())\n",
    "                else:\n",
    "                    regression_losses.append(torch.tensor(0).float())\n",
    "\n",
    "        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0,\n",
    "                                                                                                                 keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "174248ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BBoxTransform(nn.Module):\n",
    "\n",
    "    def __init__(self, mean=None, std=None):\n",
    "        super(BBoxTransform, self).__init__()\n",
    "        if mean is None:\n",
    "            self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32))\n",
    "        else:\n",
    "            self.mean = mean\n",
    "        if std is None:\n",
    "            self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32))\n",
    "        else:\n",
    "            self.std = std\n",
    "        if torch.cuda.is_available():\n",
    "            self.mean = self.mean.cuda()\n",
    "            self.std = self.std.cuda()\n",
    "\n",
    "    def forward(self, boxes, deltas):\n",
    "\n",
    "        widths = boxes[:, :, 2] - boxes[:, :, 0]\n",
    "        heights = boxes[:, :, 3] - boxes[:, :, 1]\n",
    "        ctr_x = boxes[:, :, 0] + 0.5 * widths\n",
    "        ctr_y = boxes[:, :, 1] + 0.5 * heights\n",
    "\n",
    "        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n",
    "        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n",
    "        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n",
    "        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n",
    "\n",
    "        pred_ctr_x = ctr_x + dx * widths\n",
    "        pred_ctr_y = ctr_y + dy * heights\n",
    "        pred_w = torch.exp(dw) * widths\n",
    "        pred_h = torch.exp(dh) * heights\n",
    "\n",
    "        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n",
    "        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n",
    "        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n",
    "        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n",
    "\n",
    "        return pred_boxes\n",
    "\n",
    "\n",
    "class ClipBoxes(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ClipBoxes, self).__init__()\n",
    "\n",
    "    def forward(self, boxes, img):\n",
    "        batch_size, num_channels, height, width = img.shape\n",
    "\n",
    "        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n",
    "        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n",
    "\n",
    "        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n",
    "        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n",
    "\n",
    "        return boxes\n",
    "\n",
    "\n",
    "class Anchors(nn.Module):\n",
    "    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n",
    "        super(Anchors, self).__init__()\n",
    "\n",
    "        if pyramid_levels is None:\n",
    "            self.pyramid_levels = [3, 4, 5, 6, 7]\n",
    "        if strides is None:\n",
    "            self.strides = [2 ** x for x in self.pyramid_levels]\n",
    "        if sizes is None:\n",
    "            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n",
    "        if ratios is None:\n",
    "            self.ratios = np.array([0.5, 1, 2])\n",
    "        if scales is None:\n",
    "            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    def forward(self, image):\n",
    "\n",
    "        image_shape = image.shape[2:]\n",
    "        image_shape = np.array(image_shape)\n",
    "        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n",
    "\n",
    "        all_anchors = np.zeros((0, 4)).astype(np.float32)\n",
    "\n",
    "        for idx, p in enumerate(self.pyramid_levels):\n",
    "            anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n",
    "            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n",
    "            all_anchors = np.append(all_anchors, shifted_anchors, axis=0)\n",
    "\n",
    "        all_anchors = np.expand_dims(all_anchors, axis=0)\n",
    "\n",
    "        anchors = torch.from_numpy(all_anchors.astype(np.float32))\n",
    "        if torch.cuda.is_available():\n",
    "            anchors = anchors.cuda()\n",
    "        return anchors\n",
    "\n",
    "\n",
    "def generate_anchors(base_size=16, ratios=None, scales=None):\n",
    "    if ratios is None:\n",
    "        ratios = np.array([0.5, 1, 2])\n",
    "\n",
    "    if scales is None:\n",
    "        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "    num_anchors = len(ratios) * len(scales)\n",
    "    anchors = np.zeros((num_anchors, 4))\n",
    "    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n",
    "    areas = anchors[:, 2] * anchors[:, 3]\n",
    "    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n",
    "    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n",
    "    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n",
    "    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n",
    "\n",
    "    return anchors\n",
    "\n",
    "\n",
    "def compute_shape(image_shape, pyramid_levels):\n",
    "    image_shape = np.array(image_shape[:2])\n",
    "    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n",
    "    return image_shapes\n",
    "\n",
    "\n",
    "def shift(shape, stride, anchors):\n",
    "    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n",
    "    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n",
    "    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "    shifts = np.vstack((\n",
    "        shift_x.ravel(), shift_y.ravel(),\n",
    "        shift_x.ravel(), shift_y.ravel()\n",
    "    )).transpose()\n",
    "\n",
    "    A = anchors.shape[0]\n",
    "    K = shifts.shape[0]\n",
    "    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n",
    "    all_anchors = all_anchors.reshape((K * A, 4))\n",
    "\n",
    "    return all_anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5adb5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "from efficientnet_pytorch import EfficientNet as EffNet\n",
    "from torchvision.ops.boxes import nms as nms_torch\n",
    "\n",
    "\n",
    "def nms(dets, thresh):\n",
    "    return nms_torch(dets[:, :4], dets[:, 4], thresh)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1, groups=num_channels),\n",
    "            nn.Conv2d(num_channels, num_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(num_features=num_channels, momentum=0.9997, eps=4e-5), nn.ReLU())\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv(input)\n",
    "\n",
    "\n",
    "class BiFPN(nn.Module):\n",
    "    def __init__(self, num_channels, epsilon=1e-4):\n",
    "        super(BiFPN, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        # Conv layers\n",
    "        self.conv6_up = ConvBlock(num_channels)\n",
    "        self.conv5_up = ConvBlock(num_channels)\n",
    "        self.conv4_up = ConvBlock(num_channels)\n",
    "        self.conv3_up = ConvBlock(num_channels)\n",
    "        self.conv4_down = ConvBlock(num_channels)\n",
    "        self.conv5_down = ConvBlock(num_channels)\n",
    "        self.conv6_down = ConvBlock(num_channels)\n",
    "        self.conv7_down = ConvBlock(num_channels)\n",
    "\n",
    "        # Feature scaling layers\n",
    "        self.p6_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.p5_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.p4_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.p3_upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        self.p4_downsample = nn.MaxPool2d(kernel_size=2)\n",
    "        self.p5_downsample = nn.MaxPool2d(kernel_size=2)\n",
    "        self.p6_downsample = nn.MaxPool2d(kernel_size=2)\n",
    "        self.p7_downsample = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Weight\n",
    "        self.p6_w1 = nn.Parameter(torch.ones(2))\n",
    "        self.p6_w1_relu = nn.ReLU()\n",
    "        self.p5_w1 = nn.Parameter(torch.ones(2))\n",
    "        self.p5_w1_relu = nn.ReLU()\n",
    "        self.p4_w1 = nn.Parameter(torch.ones(2))\n",
    "        self.p4_w1_relu = nn.ReLU()\n",
    "        self.p3_w1 = nn.Parameter(torch.ones(2))\n",
    "        self.p3_w1_relu = nn.ReLU()\n",
    "\n",
    "        self.p4_w2 = nn.Parameter(torch.ones(3))\n",
    "        self.p4_w2_relu = nn.ReLU()\n",
    "        self.p5_w2 = nn.Parameter(torch.ones(3))\n",
    "        self.p5_w2_relu = nn.ReLU()\n",
    "        self.p6_w2 = nn.Parameter(torch.ones(3))\n",
    "        self.p6_w2_relu = nn.ReLU()\n",
    "        self.p7_w2 = nn.Parameter(torch.ones(2))\n",
    "        self.p7_w2_relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "            P7_0 -------------------------- P7_2 -------->\n",
    "            P6_0 ---------- P6_1 ---------- P6_2 -------->\n",
    "            P5_0 ---------- P5_1 ---------- P5_2 -------->\n",
    "            P4_0 ---------- P4_1 ---------- P4_2 -------->\n",
    "            P3_0 -------------------------- P3_2 -------->\n",
    "        \"\"\"\n",
    "\n",
    "        # P3_0, P4_0, P5_0, P6_0 and P7_0\n",
    "        p3_in, p4_in, p5_in, p6_in, p7_in = inputs\n",
    "        # P7_0 to P7_2\n",
    "        # Weights for P6_0 and P7_0 to P6_1\n",
    "        p6_w1 = self.p6_w1_relu(self.p6_w1)\n",
    "        weight = p6_w1 / (torch.sum(p6_w1, dim=0) + self.epsilon)\n",
    "        # Connections for P6_0 and P7_0 to P6_1 respectively\n",
    "        p6_up = self.conv6_up(weight[0] * p6_in + weight[1] * self.p6_upsample(p7_in))\n",
    "        # Weights for P5_0 and P6_0 to P5_1\n",
    "        p5_w1 = self.p5_w1_relu(self.p5_w1)\n",
    "        weight = p5_w1 / (torch.sum(p5_w1, dim=0) + self.epsilon)\n",
    "        # Connections for P5_0 and P6_0 to P5_1 respectively\n",
    "        p5_up = self.conv5_up(weight[0] * p5_in + weight[1] * self.p5_upsample(p6_up))\n",
    "        # Weights for P4_0 and P5_0 to P4_1\n",
    "        p4_w1 = self.p4_w1_relu(self.p4_w1)\n",
    "        weight = p4_w1 / (torch.sum(p4_w1, dim=0) + self.epsilon)\n",
    "        # Connections for P4_0 and P5_0 to P4_1 respectively\n",
    "        p4_up = self.conv4_up(weight[0] * p4_in + weight[1] * self.p4_upsample(p5_up))\n",
    "\n",
    "        # Weights for P3_0 and P4_1 to P3_2\n",
    "        p3_w1 = self.p3_w1_relu(self.p3_w1)\n",
    "        weight = p3_w1 / (torch.sum(p3_w1, dim=0) + self.epsilon)\n",
    "        # Connections for P3_0 and P4_1 to P3_2 respectively\n",
    "        p3_out = self.conv3_up(weight[0] * p3_in + weight[1] * self.p3_upsample(p4_up))\n",
    "\n",
    "        # Weights for P4_0, P4_1 and P3_2 to P4_2\n",
    "        p4_w2 = self.p4_w2_relu(self.p4_w2)\n",
    "        weight = p4_w2 / (torch.sum(p4_w2, dim=0) + self.epsilon)\n",
    "        # Connections for P4_0, P4_1 and P3_2 to P4_2 respectively\n",
    "        p4_out = self.conv4_down(\n",
    "            weight[0] * p4_in + weight[1] * p4_up + weight[2] * self.p4_downsample(p3_out))\n",
    "        # Weights for P5_0, P5_1 and P4_2 to P5_2\n",
    "        p5_w2 = self.p5_w2_relu(self.p5_w2)\n",
    "        weight = p5_w2 / (torch.sum(p5_w2, dim=0) + self.epsilon)\n",
    "        # Connections for P5_0, P5_1 and P4_2 to P5_2 respectively\n",
    "        p5_out = self.conv5_down(\n",
    "            weight[0] * p5_in + weight[1] * p5_up + weight[2] * self.p5_downsample(p4_out))\n",
    "        # Weights for P6_0, P6_1 and P5_2 to P6_2\n",
    "        p6_w2 = self.p6_w2_relu(self.p6_w2)\n",
    "        weight = p6_w2 / (torch.sum(p6_w2, dim=0) + self.epsilon)\n",
    "        # Connections for P6_0, P6_1 and P5_2 to P6_2 respectively\n",
    "        p6_out = self.conv6_down(\n",
    "            weight[0] * p6_in + weight[1] * p6_up + weight[2] * self.p6_downsample(p5_out))\n",
    "        # Weights for P7_0 and P6_2 to P7_2\n",
    "        p7_w2 = self.p7_w2_relu(self.p7_w2)\n",
    "        weight = p7_w2 / (torch.sum(p7_w2, dim=0) + self.epsilon)\n",
    "        # Connections for P7_0 and P6_2 to P7_2\n",
    "        p7_out = self.conv7_down(weight[0] * p7_in + weight[1] * self.p7_downsample(p6_out))\n",
    "\n",
    "        return p3_out, p4_out, p5_out, p6_out, p7_out\n",
    "\n",
    "\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors, num_layers):\n",
    "        super(Regressor, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.header = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.layers(inputs)\n",
    "        inputs = self.header(inputs)\n",
    "        output = inputs.permute(0, 2, 3, 1)\n",
    "        return output.contiguous().view(output.shape[0], -1, 4)\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels, num_anchors, num_classes, num_layers):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.num_anchors = num_anchors\n",
    "        self.num_classes = num_classes\n",
    "        layers = []\n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n",
    "            layers.append(nn.ReLU(True))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.header = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.layers(inputs)\n",
    "        inputs = self.header(inputs)\n",
    "        inputs = self.act(inputs)\n",
    "        inputs = inputs.permute(0, 2, 3, 1)\n",
    "        output = inputs.contiguous().view(inputs.shape[0], inputs.shape[1], inputs.shape[2], self.num_anchors,\n",
    "                                          self.num_classes)\n",
    "        return output.contiguous().view(output.shape[0], -1, self.num_classes)\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(self, backbone_name):\n",
    "        super(EfficientNet, self).__init__()\n",
    "        model = EffNet.from_pretrained(backbone_name)\n",
    "        del model._conv_head\n",
    "        del model._bn1\n",
    "        del model._avg_pooling\n",
    "        del model._dropout\n",
    "        del model._fc\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model._swish(self.model._bn0(self.model._conv_stem(x)))\n",
    "        feature_maps = []\n",
    "        for idx, block in enumerate(self.model._blocks):\n",
    "            drop_connect_rate = self.model._global_params.drop_connect_rate\n",
    "            if drop_connect_rate:\n",
    "                drop_connect_rate *= float(idx) / len(self.model._blocks)\n",
    "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
    "            if block._depthwise_conv.stride == [2, 2]:\n",
    "                feature_maps.append(x)\n",
    "\n",
    "        return feature_maps[1:]\n",
    "\n",
    "\n",
    "class EfficientDet(nn.Module):\n",
    "    def __init__(self, num_anchors=9, num_classes=20, compound_coef=0, backbone_name='efficientnet-b0'):\n",
    "        super(EfficientDet, self).__init__()\n",
    "        self.compound_coef = compound_coef\n",
    "\n",
    "        self.num_channels = [64, 88, 112, 160, 224, 288, 384, 384][self.compound_coef]\n",
    "\n",
    "        self.conv3 = nn.Conv2d(40, self.num_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(80, self.num_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv5 = nn.Conv2d(192, self.num_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv6 = nn.Conv2d(192, self.num_channels, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(48, self.num_channels, kernel_size=1, stride=1, padding=0)\n",
    "#         self.conv4 = nn.Conv2d(96, self.num_channels, kernel_size=1, stride=1, padding=0)\n",
    "#         self.conv5 = nn.Conv2d(232, self.num_channels, kernel_size=1, stride=1, padding=0)\n",
    "#         self.conv6 = nn.Conv2d(232, self.num_channels, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.conv7 = nn.Sequential(nn.ReLU(),\n",
    "                                   nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        self.bifpn = nn.Sequential(*[BiFPN(self.num_channels) for _ in range(min(2 + self.compound_coef, 8))])\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.regressor = Regressor(in_channels=self.num_channels, num_anchors=num_anchors,\n",
    "                                   num_layers=3 + self.compound_coef // 3)\n",
    "        self.classifier = Classifier(in_channels=self.num_channels, num_anchors=num_anchors, num_classes=num_classes,\n",
    "                                     num_layers=3 + self.compound_coef // 3)\n",
    "\n",
    "        self.anchors = Anchors()\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "        self.clipBoxes = ClipBoxes()\n",
    "        self.focalLoss = FocalLoss()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        prior = 0.01\n",
    "\n",
    "        self.classifier.header.weight.data.fill_(0)\n",
    "        self.classifier.header.bias.data.fill_(-math.log((1.0 - prior) / prior))\n",
    "\n",
    "        self.regressor.header.weight.data.fill_(0)\n",
    "        self.regressor.header.bias.data.fill_(0)\n",
    "\n",
    "        self.backbone_net = EfficientNet(backbone_name=backbone_name)\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if len(inputs) == 2:\n",
    "            is_training = True\n",
    "            img_batch, annotations = inputs\n",
    "        else:\n",
    "            is_training = False\n",
    "            img_batch = inputs\n",
    "\n",
    "        c3, c4, c5 = self.backbone_net(img_batch)\n",
    "        \n",
    "        p3 = self.conv3(c3)\n",
    "        p4 = self.conv4(c4)\n",
    "        p5 = self.conv5(c5)\n",
    "        p6 = self.conv6(c5)\n",
    "        p7 = self.conv7(p6)\n",
    "\n",
    "        features = [p3, p4, p5, p6, p7]\n",
    "        features = self.bifpn(features)\n",
    "\n",
    "        regression = torch.cat([self.regressor(feature) for feature in features], dim=1)\n",
    "        classification = torch.cat([self.classifier(feature) for feature in features], dim=1)\n",
    "        anchors = self.anchors(img_batch)\n",
    "\n",
    "        if is_training:\n",
    "            return self.focalLoss(classification, regression, anchors, annotations)\n",
    "        else:\n",
    "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
    "            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n",
    "\n",
    "            scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
    "\n",
    "            scores_over_thresh = (scores > 0.05)[0, :, 0]\n",
    "\n",
    "            if scores_over_thresh.sum() == 0:\n",
    "                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
    "\n",
    "            classification = classification[:, scores_over_thresh, :]\n",
    "            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
    "            scores = scores[:, scores_over_thresh, :]\n",
    "\n",
    "            anchors_nms_idx = nms(torch.cat([transformed_anchors, scores], dim=2)[0, :, :], 0.5)\n",
    "\n",
    "            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n",
    "\n",
    "            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce56e80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 20506\n",
      "Val size: 5127\n"
     ]
    }
   ],
   "source": [
    "root_path = 'vkcv2022-contest-02-carplates/data/'\n",
    "json_path = 'vkcv2022-contest-02-carplates/data/train.json'\n",
    "val_size = 0.2\n",
    "\n",
    "train_transform = A.Compose([A.Resize(640, 640),\n",
    "                            A.HorizontalFlip(p=0.25),\n",
    "                            A.Rotate(limit=(-20, 20), p=0.25),\n",
    "                            A.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.2, hue=0.05, p=0.25)],\n",
    "                            bbox_params={\n",
    "                                        'format': 'pascal_voc',\n",
    "                                        'label_fields': ['labels']\n",
    "                                        })\n",
    "\n",
    "\n",
    "val_transform = A.Compose([A.Resize(640, 640)],\n",
    "                            bbox_params={\n",
    "                                        'format': 'pascal_voc',\n",
    "                                        'label_fields': ['labels']\n",
    "                                        })\n",
    "\n",
    "trainset = PlatesDataset(root_path=root_path, json_path=json_path, sample_type='train', val_size=val_size, \n",
    "                         transform=train_transform)\n",
    "valset = PlatesDataset(root_path=root_path, json_path=json_path, sample_type='val', val_size=val_size,\n",
    "                      transform=val_transform)\n",
    "\n",
    "\n",
    "\n",
    "print('Train size:', len(trainset))\n",
    "print('Val size:', len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e469ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "def get_iou(true_box, pred_box):\n",
    "    \n",
    "    ix1 = np.maximum(true_box[0], pred_box[0])\n",
    "    iy1 = np.maximum(true_box[1], pred_box[1])\n",
    "    ix2 = np.minimum(true_box[2], pred_box[2])\n",
    "    iy2 = np.minimum(true_box[3], pred_box[3])\n",
    "    \n",
    "    i_height = np.maximum(iy2 - iy1 + 1, np.array(0.))\n",
    "    i_width = np.maximum(ix2 - ix1 + 1, np.array(0.))\n",
    "    \n",
    "    area_of_intersection = i_height * i_width\n",
    "    \n",
    "    gt_height = true_box[3] - true_box[1] + 1\n",
    "    gt_width = true_box[2] - true_box[0] + 1\n",
    "    \n",
    "    pd_height = pred_box[3] - pred_box[1] + 1\n",
    "    pd_width = pred_box[2] - pred_box[0] + 1\n",
    "    \n",
    "    area_of_union = gt_height * gt_width + pd_height * pd_width - area_of_intersection\n",
    "    \n",
    "    iou = area_of_intersection / area_of_union\n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "class ModelTrainer():\n",
    "    \n",
    "    def __init__(self, model, model_name, train_loader, val_loader, optimizer, device, scheduller=None,\n",
    "                 conf_thresh=0.5, val_classes=None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduller = scheduller\n",
    "        \n",
    "        self.conf_thresh = conf_thresh\n",
    "        self.val_classes = val_classes\n",
    "        \n",
    "        \n",
    "    def fit_epoch(self):\n",
    "        \n",
    "        obj_loss = []\n",
    "        reg_loss = []\n",
    "        global_loss = []\n",
    "        \n",
    "        self.model.train()\n",
    "        for idx, data in tqdm(enumerate(self.train_loader)):\n",
    "            \n",
    "            imgs = data['img'].to(device)\n",
    "            targets = data['annot'].to(device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                losses = self.model((imgs, targets))\n",
    "            except Exception as e:\n",
    "                print('Error in training step')\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            loss = sum([v for v in losses])\n",
    "            loss.backward()\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            if self.scheduller is not None:\n",
    "                self.scheduller.step()            \n",
    "            \n",
    "            global_loss.append(loss.item())\n",
    "            obj_loss.append(losses[0].item())\n",
    "            reg_loss.append(losses[1].item())\n",
    "            \n",
    "            if idx % 200 == 0:\n",
    "                print('Global Loss:', np.mean(global_loss))\n",
    "                print('Cls Loss:', np.mean(obj_loss))\n",
    "                print('Reg Loss:', np.mean(reg_loss))\n",
    "            \n",
    "        return np.mean(global_loss)\n",
    "    \n",
    "    \n",
    "    def eval_sample(self, pred, target, iou_thresh, result_dict):\n",
    "        \n",
    "        pred_bboxes = pred[2].cpu().numpy()\n",
    "        scores = pred[0].cpu().numpy()\n",
    "        pred_labels = pred[1].cpu().numpy()\n",
    "        \n",
    "        true_bboxes = target[0][:,:4].cpu().numpy()\n",
    "        true_labels = target[0][:, 4].cpu().numpy()\n",
    "        \n",
    "        for i in range(len(pred_bboxes)):\n",
    "            \n",
    "            pred_box = pred_bboxes[i]\n",
    "            pred_label = pred_labels[i]\n",
    "            \n",
    "            result_dict['pred_label'].append(pred_label)\n",
    "            result_dict['score'].append(scores[i])\n",
    "            \n",
    "            max_iou = 0\n",
    "            max_id = -1\n",
    "            for j in range(len(true_bboxes)):\n",
    "                \n",
    "                true_box = true_bboxes[j]\n",
    "                true_label = true_labels[j]\n",
    "                \n",
    "                if true_label != pred_label:\n",
    "                    continue\n",
    "                       \n",
    "                IoU = get_iou(true_box, pred_box)\n",
    "                if (IoU >= iou_thresh) and (IoU > max_iou):\n",
    "                    max_iou = IoU\n",
    "                    max_id = j\n",
    "            \n",
    "            if max_id >= 0:\n",
    "                result_dict['TP'].append(1)\n",
    "                true_bboxes = np.delete(true_bboxes, max_id, axis=0)\n",
    "                true_labels = np.delete(true_labels, max_id, axis=0)\n",
    "                \n",
    "            else:\n",
    "                result_dict['TP'].append(0)\n",
    "                \n",
    "            \n",
    "        return result_dict\n",
    "    \n",
    "    \n",
    "    def compute_metrics(self, predictions, targets):\n",
    "        \n",
    "\n",
    "        iou_list_results = {str(round(iou, 2)): {class_id: {'precision_list': [], 'recall_list': []} \n",
    "                                                 for class_id in self.val_classes} \n",
    "                            for iou in np.linspace(0.5, 0.95, 10)}\n",
    "        result_metrics = {'map_list': [], 'map@50': 0, 'map@50_95': 0, 'precsion': 0, 'recall': 0}\n",
    "        \n",
    "        for iou in tqdm(iou_list_results):\n",
    "            result_dict = {'pred_label': [], 'score': [], 'TP': []}\n",
    "            n_boxes = {label: 0 for label in self.val_classes}\n",
    "        \n",
    "            for pred, true in zip(predictions, targets):\n",
    "                result_dict = self.eval_sample(pred, true, float(iou), result_dict)\n",
    "                for label in true[0]:\n",
    "                    n_boxes[label[4].item()] += 1\n",
    "        \n",
    "            df = pd.DataFrame(result_dict)\n",
    "            iou_list_results[iou]['ap_list'] = []\n",
    "            \n",
    "            if iou == '0.5':\n",
    "                iou_list_results[iou]['precision_list'] = []\n",
    "                iou_list_results[iou]['recall_list'] = []\n",
    "            \n",
    "            for class_id in self.val_classes:\n",
    "                df_per_class = df[df['pred_label'] == class_id]\n",
    "                df_per_class = df_per_class.sort_values(by='score', ascending=False)\n",
    "                \n",
    "                \n",
    "                TP = 0\n",
    "                preds_count = 0\n",
    "                for idx, row in df_per_class.iterrows():\n",
    "                    TP += row['TP']\n",
    "                    preds_count += 1\n",
    "                    \n",
    "                    precision = TP/preds_count\n",
    "                    recall = TP/n_boxes[class_id]\n",
    "                    \n",
    "                    iou_list_results[iou][class_id]['precision_list'].append(precision)\n",
    "                    iou_list_results[iou][class_id]['recall_list'].append(recall)\n",
    "                    \n",
    "                    if (iou == '0.5') and (row['score'] >= self.conf_thresh-0.02) and (row['score'] <= self.conf_thresh+0.02):\n",
    "                        iou_list_results[iou][class_id]['precision'] = precision\n",
    "                        iou_list_results[iou][class_id]['recall'] = recall\n",
    "                \n",
    "                \n",
    "                iou_list_results[iou][class_id]['AP'] = auc(iou_list_results[iou][class_id]['recall_list'],\n",
    "                                                                iou_list_results[iou][class_id]['precision_list'])\n",
    "                \n",
    "                iou_list_results[iou]['ap_list'].append(iou_list_results[iou][class_id]['AP'])\n",
    "                \n",
    "                if iou == '0.5':\n",
    "                    try:\n",
    "                        iou_list_results[iou]['precision_list'].append(iou_list_results[iou][class_id]['precision'])\n",
    "                        iou_list_results[iou]['recall_list'].append(iou_list_results[iou][class_id]['recall'])\n",
    "                    except:\n",
    "                        iou_list_results[iou]['precision_list'].append(0)\n",
    "                        iou_list_results[iou]['recall_list'].append(0)\n",
    "            \n",
    "        for iou in iou_list_results:\n",
    "            if iou == '0.5':\n",
    "                result_metrics['precision'] = np.mean(iou_list_results[iou]['precision_list'])\n",
    "                result_metrics['recall'] = np.mean(iou_list_results[iou]['recall_list'])\n",
    "            \n",
    "            result_metrics['map_list'].append(np.mean(iou_list_results[iou]['ap_list']))\n",
    "        \n",
    "        result_metrics['map@50'] = result_metrics['map_list'][0]\n",
    "        result_metrics['map@50_95'] = np.mean(result_metrics['map_list'])\n",
    "        \n",
    "            \n",
    "        return result_metrics\n",
    "            \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def eval_epoch(self):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        predictions = []\n",
    "        answers = []\n",
    "        for idx, data in tqdm(enumerate(self.val_loader)):\n",
    "            \n",
    "            imgs = data['img'].to(device)\n",
    "            targets = data['annot'].to(device)\n",
    "            \n",
    "            outputs = self.model(imgs)\n",
    "            \n",
    "            \n",
    "            predictions.append(outputs)\n",
    "            answers.append(targets)\n",
    "            \n",
    "        result = self.compute_metrics(predictions, answers)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def train_net(self, num_epochs):\n",
    "        \n",
    "        best_map = 0\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            \n",
    "            train_loss = self.fit_epoch()\n",
    "            result_metrics = self.eval_epoch()\n",
    "            \n",
    "            if result_metrics['map@50_95'] >= best_map:\n",
    "                best_map = result_metrics['map@50_95']\n",
    "                torch.save(self.model, f'SaveModels/{self.model_name}_{epoch}.pth')\n",
    "            \n",
    "            with open(f'Logs/{self.model_name}.txt', 'a') as f:\n",
    "                string = f\"Precision={result_metrics['precision']} Recall={result_metrics['recall']} MAP50={result_metrics['map@50']} MAP50_95={result_metrics['map@50_95']}\\n\"\n",
    "                f.write(string)\n",
    "            \n",
    "            print('Epoch:', epoch)\n",
    "            print('Precsion:', result_metrics['precision'])\n",
    "            print('Recall:', result_metrics['recall'])\n",
    "            print('MAP@50:', result_metrics['map@50'])\n",
    "            print('MAP@50_95:', result_metrics['map@50_95'])\n",
    "            \n",
    "                        \n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "efficient_det_b3 = EfficientDet(num_classes=2, backbone_name='efficientnet-b0')\n",
    "    \n",
    "for param in efficient_det_b3.backbone_net.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for param in efficient_det_b3.backbone_net.model._blocks[15].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in efficient_det_b3.backbone_net.model._blocks[14].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "for param in efficient_det_b3.backbone_net.model._blocks[13].parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "    \n",
    "for name, param in efficient_det_b3.named_parameters():\n",
    "    print(name, param.requires_grad)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "model_name = 'efficientnet_b0_v2'\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "num_epochs = 20\n",
    "lr = 3e-4\n",
    "batch_size = 16\n",
    "num_workers = 8\n",
    "\n",
    "#efficient_det_b3 = nn.DataParallel(efficient_det_b3, device_ids=[0, 2, 4, 6])\n",
    "\n",
    "efficient_det_b3.to(device)\n",
    "optimizer = torch.optim.AdamW(efficient_det_b3.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, \n",
    "                          collate_fn=collater, num_workers=num_workers)\n",
    "val_loader = DataLoader(valset, batch_size=1, shuffle=False, \n",
    "                        collate_fn=collater, num_workers=num_workers)\n",
    "\n",
    "\n",
    "scheduller = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=lr, steps_per_epoch=len(train_loader), \n",
    "                                                epochs=num_epochs, pct_start=0.05, anneal_strategy='cos')\n",
    "\n",
    "\n",
    "model_trainer = ModelTrainer(model=efficient_det_b3, model_name=model_name, train_loader=train_loader, val_loader=val_loader,\n",
    "                            optimizer=optimizer, scheduller=scheduller, device=device, val_classes=[1])\n",
    "\n",
    "model_trainer.train_net(num_epochs=num_epochs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
